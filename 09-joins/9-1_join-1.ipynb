{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "# Join 1\n",
    "\n",
    "## Overview\n",
    "\n",
    "Straight up join\n",
    "\n",
    "## Duration\n",
    "\n",
    "20 mins\n",
    "\n",
    "## Step-1: Verify datsets\n",
    "\n",
    "We have 2 datasets\n",
    "\n",
    "- transactions data (large data).  Sample data is in `data/transactions/transactions-sample.csv`\n",
    "- rewards data (small data).  Sample data in `data/reward-points/reward-points.csv`\n",
    "\n",
    "Both datasets have `merchant_id` field in common.\n",
    "\n",
    "Also optionally, verify you have this data in HDFS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Start up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    import findspark\n",
    "    findspark.init()  # uses SPARK_HOME\n",
    "    print(\"Spark found in : \", findspark.find())\n",
    "\n",
    "    import pyspark\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # use a unique tmep dir for warehouse dir, so we can run multiple spark sessions in one dir\n",
    "    import tempfile\n",
    "    tmpdir = tempfile.TemporaryDirectory()\n",
    "\n",
    "    config = ( SparkConf()\n",
    "             .setAppName(\"TestApp\")\n",
    "             .setMaster(\"local[*]\")\n",
    "             .set('executor.memory', '2g')\n",
    "             .set('spark.sql.warehouse.dir', tmpdir.name)\n",
    "             .set('spark.sql.adaptive.enabled', 'true')\n",
    "             .set('spark.sql.adaptive.coalescePartitions.enabled', 'true')\n",
    "             )\n",
    "\n",
    "    spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "print('Spark UI running on port ' + spark.sparkContext.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if AQE is enabled\n",
    "spark.conf.get('spark.sql.adaptive.enabled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Load both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "## TODO: Adjust the data path accordingly\n",
    "transactions = spark.read.csv(\"/home/ubuntu/dev/data-pipelining-with-spark-labs-101/data/transactions/csv\", header=True)\n",
    "t2 = time.perf_counter()  \n",
    "\n",
    "## TODO: Adjust the data path accordingly\n",
    "rewards = spark.read.csv(\"/home/ubuntu/dev/data-pipelining-with-spark-labs-101/data/reward-points/reward-points.csv\", header=True)\n",
    "t3 = time.perf_counter()\n",
    "\n",
    "print (\"Loaded transactions data in {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "print (\"Loaded rewward points data in {:,.2f} ms \".format( (t3-t2)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Register Tables\n",
    "\n",
    "Let's register the tables, so we can use spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "rewards.createOrReplaceTempView(\"rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Quick Check on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from rewards\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: Join\n",
    "\n",
    "Join both datasets using `merchant_id` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "SELECT transactions.*, rewards.*  \n",
    "from transactions join rewards \n",
    "ON (transactions.merchant_id = rewards.merchant_id)\n",
    "\"\"\"\n",
    "\n",
    "joined = spark.sql(s)\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want a better display, convert to pandas df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "joined.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-7: Join Aggregate Query\n",
    "\n",
    "We will expand on the previous join query, and do an aggregate.\n",
    "\n",
    "We will calculate total rewards points per merchant.\n",
    "\n",
    "`total reward points = money spent * rewards per dollar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shape joined data\n",
    "## since we have two 'marchant_id's, let's only keep one.  \n",
    "## And we have to qualify it with the table name\n",
    "joined2 = joined.select(['rewards.merchant_id', 'amount_customer', 'reward_points' ])\n",
    "joined2.createOrReplaceTempView(\"joined2\")\n",
    "joined2.sample(0.1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the total rewards per merchant\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "s = \"\"\"\n",
    "SELECT merchant_id, \n",
    "       SUM(amount_customer) as total_spend,\n",
    "       SUM(amount_customer * reward_points) as total_rewards\n",
    "from joined2\n",
    "group by merchant_id\n",
    "order by total_rewards desc\n",
    "\"\"\"\n",
    "\n",
    "rewards_summary = spark.sql(s)\n",
    "rewards_summary.show(5)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print (\"Join in {:,.2f} ms \".format( (t2-t1)*1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format number display\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "rewards_summary2 = rewards_summary.\\\n",
    "                         withColumn ('total_spend', F.format_number('total_spend', 0)).\\\n",
    "                         withColumn ('total_rewards', F.format_number('total_rewards', 0))\n",
    "\n",
    "rewards_summary2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-8: See the query plan\n",
    "\n",
    "Use `explain` keyword.\n",
    "\n",
    "Can you spot any optimizations?\n",
    "\n",
    "Hint : Look at the physical plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = spark.sql(s)\n",
    "joined.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-9: See the DAG on Spark UI\n",
    "\n",
    "Go to Spark UI and observe the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-10: Now Run this on Hadoop cluster\n",
    "\n",
    "Launch spark on Hadoop cluster, and load both datasets and do a join.\n",
    "\n",
    "Here is some sample code to get you started.  Adjust TODO items as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Adjust data paths accordingly\n",
    "transactions = spark.read.csv(\"/user/me/transactions/csv\", header=True)\n",
    "rewards = spark.read.csv(\"/user/me/reward-points/reward-points.csv\", header=True)\n",
    "\n",
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "rewards.createOrReplaceTempView(\"rewards\")\n",
    "\n",
    "s = \"\"\"\n",
    "SELECT transactions.merchant_id, count(*) as total_rewards\n",
    "from transactions join rewards \n",
    "ON (transactions.merchant_id= rewards.merchant_id)\n",
    "group by transactions.merchant_id\n",
    "order by total_rewards desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(s).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
